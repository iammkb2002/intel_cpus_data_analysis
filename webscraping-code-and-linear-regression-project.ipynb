{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# INTEL CPUs REGRESSION PROJECT","metadata":{}},{"cell_type":"markdown","source":"Let's start by importing the required libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport requests\nimport os\nfrom bs4 import BeautifulSoup as bs\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Webscraping","metadata":{}},{"cell_type":"markdown","source":"First we need to setup our selenium webdriver...","metadata":{}},{"cell_type":"code","source":"opts = webdriver.ChromeOptions()\nopts.binary_location = os.environ.get('GOOGLE_CHROME_BIN', None)\nopts.add_argument(\"--headless\")\nopts.add_argument(\"--disable-dev-shm-usage\")\nopts.add_argument(\"--no-sandbox\")\ndriver = webdriver.Chrome(options=opts)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first page has all the links to every CPUs generation, we need data for all of them.","metadata":{}},{"cell_type":"code","source":"url = \"https://ark.intel.com/content/www/us/en/ark.html#@PanelLabel122139\"\nreq = requests.get(url).text\npage = bs(req, \"html.parser\")\n\nhrefs = page.find(\"div\", class_=\"products processors\", attrs={\"data-parent-panel-key\":\"PanelLabel122139\", \"style\":\"display: none;\"}).find_all(\"a\", href=True)\nclasses_links = []\nfor i in range(len(hrefs)):  \n    classes_links.append(hrefs[i][\"href\"])","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each stored link has in turn an hyper reference to each specific CPU model. By storing them in a list and checking its lenght we can assess how many they are.","metadata":{}},{"cell_type":"code","source":"cpu_links=[]\nfor url in classes_links:\n    class_url = \"https://ark.intel.com\" + url\n    req1 = requests.get(class_url).text\n    page1 = bs(req1, \"html.parser\")\n    hrefs1 = page1.find(\"tbody\").find_all(\"a\")\n    for i in range(len(hrefs1)):\n        cpu_links.append(hrefs1[i][\"href\"])\n\nlen(cpu_links)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A little more than a thousand! That will do for a small dataset. Now we need to extract all the relevant features for every CPU...","metadata":{}},{"cell_type":"code","source":"features = [\"MarketSegment\", \"Lithography\", \"CoreCount\", \"ThreadCount\", \"ClockSpeedMax\", \"TurboBoostMaxTechMaxFreq\", \"ClockSpeed\", \"Cache\",\n            \"Bus\", \"MaxTDP\", \"BornOnDate\"]\ndata = []\n\nk = 0\nfor l in cpu_links:\n    cpu_url = \"https://ark.intel.com\" + l\n    driver.get(cpu_url)\n    page2 = bs(driver.page_source, \"html.parser\")\n    \n    try:\n        cpu_model = page2.find(\"h1\", class_=\"h1\").string.strip()\n        data.append(cpu_model)\n    except:\n        data.append(pd.NA)\n    \n    check_price = page2.find(\"a\", class_=\"view-modal info-modal\", attrs={\"data-modal\":\"tt-recommendedCustomerPrice\"}) \n    if check_price is None:\n        data.append(pd.NA)\n    else:\n        price = page2.find(\"span\", class_=\"value\", attrs={\"data-key\":\"\"}).string\n        data.append(price)\n\n    for feature in features:\n        try:\n            f = page2.find(\"span\", class_=\"value\", attrs={\"data-key\":feature}).string.strip()\n            data.append(f)\n        except:\n            data.append(pd.NA)\n    k += 1\n    if k%100==0:\n        print(f\"Product %d out of %d completed\"%(k, len(cpu_links)))","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We scraped all the data we needed, time to store them into a pandas dataframe. We'll also export it as a csv file as a backup copy.","metadata":{}},{"cell_type":"code","source":"data = np.asarray(data).reshape(-1, 13)\ndf = pd.DataFrame(data)\ndf.columns = [\"Model\", \"Price\"] + features\n\ndf.to_csv(\"D:/Utente/Documenti/datasets/intel.csv\")","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Processing","metadata":{}},{"cell_type":"markdown","source":"Let's take a quick glance to our data: ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"D:/Utente/Documenti/datasets/intel.csv\")\ndf.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can drop TurboBoost since almost all values are null. All the rows where the price is missing have to be dropped as well since they're too many for an imputation.","metadata":{}},{"cell_type":"code","source":"df.drop(\"TurboBoostMaxTechMaxFreq\", axis=1, inplace=True)\ndf.drop(\"Unnamed: 0\", axis=1, inplace=True)\ndf.dropna(inplace=True, subset=\"Price\")\n\ndf.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Almost all variables are classified as object types. They have to be converted into numerical ones.\nThe following function convert prices into float objects. If they are reported as ranges, it returns the central value.","metadata":{}},{"cell_type":"code","source":"df.reset_index(inplace=True)\ndf.drop(\"index\", axis=1, inplace=True)\n\ndef convert_price(price):\n    try:\n        values = price.split(' - ')\n        min_value = float(values[0].replace('$', ''))\n        max_value = float(values[1].replace('$', ''))\n        return (min_value + max_value) / 2\n    \n    except:\n        value = np.float64(price.replace(\"$\", \"\"))\n        return value\n    \n\ndf[\"Price\"] = df[\"Price\"].apply(convert_price)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The variable MarketSegment has only three different values. To convert them into numbers we can resort to One Hot Encoding.","metadata":{}},{"cell_type":"code","source":"ohe = pd.get_dummies(df.MarketSegment)\ndf = df.join(ohe)\ndf.drop(\"MarketSegment\", axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df[\"Lithography\"].unique())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to this website https://www.hpcwire.com/2021/07/27/intels-new-node-names-sapphire-rapids-now-an-intel-7-chip/, Intel 7 lithography corresponds to 10 nm. We'll use this information to make an imputation.","metadata":{}},{"cell_type":"code","source":"df.loc[df[\"Lithography\"] == \"Intel 7\", \"Lithography\"] = 10\nfor i in range(df.shape[0]):\n    try:\n        df.loc[i, \"Lithography\"] = np.int8(df.loc[i, \"Lithography\"][:2])\n    except:\n        continue\n    \ndf[\"Lithography\"] = pd.to_numeric(df[\"Lithography\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ClockSpeedMax, ClockSpeed must to be turned into numerical types as well.","metadata":{}},{"cell_type":"code","source":"print(df[\"ClockSpeedMax\"].unique())\n\ndf[\"ClockSpeedMax\"] = pd.to_numeric(df[\"ClockSpeedMax\"].str.replace(\" GHz\", \"\"), errors='coerce')\n\n\nprint(df[\"ClockSpeed\"].unique())\n\ndf[\"ClockSpeed\"] = df[\"ClockSpeed\"].str[:-4]\ndf[\"ClockSpeed\"] = pd.to_numeric(df[\"ClockSpeed\"], errors='coerce')\n\n# Convert MHz to GHz\ndf.loc[df[\"ClockSpeed\"] > 100, \"ClockSpeed\"] /= 1000","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As for the Cache, we can distinguish 4 types: basic, L2, L3 and Intel® Smart Cache. Since their performance gets better as the technlogy progresses, we can use a label encoder for them: the most recent the technology the higher the label.","metadata":{}},{"cell_type":"code","source":"for i in range(df.shape[0]):\n    if \"L2\" in df.iloc[i, 7]:\n        df.loc[i, \"Cache\"] = 1\n    elif \"L3\" in df.iloc[i, 7]:\n        df.loc[i, \"Cache\"] = 2\n    elif \"Intel® Smart Cache\" in df.iloc[i, 7]:\n        df.loc[i, \"Cache\"] = 3\n    else:\n        df.loc[i, \"Cache\"] = 0\n        \ndf[\"Cache\"] = pd.to_numeric(df[\"Cache\"])\n\nprint(df[\"Cache\"].unique())","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Bus variable is hard to standardize since Gigatransfers and Megahertz are not convertible into eachother. Might as well drop it.","metadata":{"tags":[]}},{"cell_type":"code","source":"df.drop(\"Bus\", axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert MaxTDP into int type as well.","metadata":{}},{"cell_type":"code","source":"df[\"MaxTDP\"] = pd.to_numeric(df[\"MaxTDP\"].str.replace(\" W\", \"\"), errors='coerce')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The only variable not yet covered is the date of release. Since a time variable wouldn't fit into a price prediction model, we can just forget about it.","metadata":{}},{"cell_type":"code","source":"df.drop(\"BornOnDate\", axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have converted all of our data into a workable format let's take a look at the bigger picture.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We still have many null values, especially for MaxTDP and ClockSpeed. Let's check the descriptive statistics.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clockspeed's standard deviation isn't exaggerately high compared to its mean: that's good news since it allows us to substitute null values with the median without losing too many informations. On the other hand, MaxTDP presents both a very high SD and a very large interquartile range, it would be risky to make imputations with respect to null values. We can conclude that the best option is to just drop it.","metadata":{}},{"cell_type":"markdown","source":"As for the null values in the other variables, they're so few that it is safe to assume imputations won't do any harm. We choose to substitute them with the median since they belong to a small dataset, where the mean can easily be distorted by outliers.","metadata":{}},{"cell_type":"code","source":"df.drop(\"MaxTDP\", axis=1, inplace=True)\n\nimputations = df.columns[df.isnull().any()]\n\nfor i in imputations:\n    df[i].fillna(df[i].median(), inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's done! The data has been processed: all of our variables are now in the correct data type and we don't have any null values. Let's check one more time to be sure.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Analysis and Visualization","metadata":{}},{"cell_type":"markdown","source":"Now for some data analysis. Let's start by plotting a correlation matrix.","metadata":{}},{"cell_type":"code","source":"data = df.iloc[:, 1:]\nsns.heatmap(data.corr(), annot=True, fmt=\".2f\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlation between the number of cores and the number of threads is next to 1. In order to avoid multicollinearity in our regression model we can exclude one of them.","metadata":{}},{"cell_type":"code","source":"data.drop(\"ThreadCount\", axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Other interesting graphs...","metadata":{}},{"cell_type":"code","source":"data['MarketSegment'] = data[['Desktop', 'Mobile', 'Embedded']].idxmax(axis=1)\n\n# Create a 2x2 subplot\nfig, ax = plt.subplots(2, 2, figsize=(10, 8))\n\n# Plot scatter plots on the first row\nax[0, 0].scatter(data[\"CoreCount\"], data[\"Price\"])\nax[0, 0].set_xticks(pd.unique(data[\"CoreCount\"]))\nax[0, 0].set_xlabel('Core Count')\nax[0, 0].set_ylabel('Price')\nax[0, 0].set_title('Scatter Plot: Core Count vs. Price')\n\nax[0, 1].scatter(data[\"ClockSpeedMax\"], data[\"Price\"])\nax[0, 1].set_xlabel('Clock Speed Max')\nax[0, 1].set_ylabel('Price')\nax[0, 1].set_title('Scatter Plot: Clock Speed Max vs. Price')\n\n# Plot the boxplot on the second row\nsns.boxplot(x='MarketSegment', y='Price', data=data, ax=ax[1, 0], hue='MarketSegment')\nax[1, 0].set_xlabel('Market Segment')\nax[1, 0].set_ylabel('Price')\nax[1, 0].set_title('Boxplot of Price for Different Market Segments')\n\n# Plot the histogram on the second row\nsns.histplot(data=data, x='MarketSegment', stat='count', ax=ax[1, 1])\nax[1, 1].set_xlabel('Market Segment')\nax[1, 1].set_ylabel('Count')\nax[1, 1].set_title('Histogram of Market Segments')\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plots\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears to be a positive relationship between the price and both the number of cores and the maximum clock speed. We can try to predict the price of a CPU through a regression model using these variables.","metadata":{}},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"markdown","source":"From the CoreCount vs Price plot it would seem that the relationship between the two has a linear shape. On the other hand, the ClockSpeedMax vs Price plot suggests that the relationship in this case may be curvilinear.  ","metadata":{}},{"cell_type":"code","source":"# This function will plot our results\ndef lin_regplot(x, y, model):\n    plt.scatter(x, y, c=\"steelblue\", edgecolor=\"white\", s=70)\n    plt.plot(x, model.predict(x), color=\"red\", lw=2, linestyle='-', \n             label=f\"Slope: {model.coef_[0][0]:.4f} \\nIntercept: {model.intercept_[0]:.4f}\")\n    plt.legend()\n    return None\n\nx1 = np.asarray(data[\"CoreCount\"]).reshape(-1, 1)\nx2 =  np.asarray(data[\"ClockSpeedMax\"]).reshape(-1, 1)\ny =  np.asarray(data[\"Price\"]).reshape(-1, 1)\n\n# First model: predict price through the number of cores \nlr1 = LinearRegression()\nlr1.fit(x1, y)\nplt.title(\"Price vs CoreCount Regression\")\nlin_regplot(x1, y, lr1)\nplt.xlabel(\"CoreCount\")\nplt.ylabel(\"Price\")\nplt.tight_layout()\nplt.show()\n\n# Second model: predict price through the maximum clock speed\n# let's start with a simple linear regression...\nx_fit = np.linspace(min(x2), max(x2), 100)\nlr2 = LinearRegression()\nlr2.fit(x2, y)\ny_lin_fit = lr2.predict(x_fit)\n# ... then estimate a 2nd order polinomial one\nlr3 = LinearRegression()\nquad = PolynomialFeatures(degree=2)\nx2_quad = quad.fit_transform(x2)\nlr3.fit(x2_quad, y)\ny_quad_fit = lr3.predict(quad.fit_transform(x_fit))\n\nplt.scatter(x2, y, c=\"steelblue\", edgecolor=\"white\", s=70)\nplt.plot(x_fit, y_lin_fit, label=f\"Slope: {lr2.coef_[0][0]:.4f} \\nIntercept: {lr2.intercept_[0]:.4f}\",\n        linestyle=\"--\", lw=2, color=\"red\")\nplt.plot(x_fit, y_quad_fit, label=\"Quadratic regression\", color = \"magenta\", lw=2)\nplt.xlabel(\"ClockSpeedMax\")\nplt.ylabel(\"Price\")\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both models seem to confirm what the exploratory analysis was already suggesting: the relationships between the price and the cores and the maximum clock speed are significantly positive. Also, it appears the intuiton we had about ClockSpeedMax was right: a quadratic fit seems to give better results than a simple linear regression. ","metadata":{}},{"cell_type":"markdown","source":"Since both explanatory variables seem good predictors for the price, it's a good idea to include both of them in a final regression model. We'll also need to know how accurately the final model will predict prices. To find that out we can perform a train test split.","metadata":{}},{"cell_type":"code","source":"X = data[[\"ClockSpeedMax\", \"CoreCount\"]].values\nY = data[\"Price\"].values\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=0, shuffle=True)\nprint(f\"X train: {X_train.shape}\")\nprint(f\"X test: {X_test.shape}\")\nprint(f\"Y train: {Y_train.shape}\")\nprint(f\"Y test: {Y_test.shape}\")\n\n# Now build the model\nlr4 = LinearRegression()\nlr4.fit(X_train, Y_train)\ny_train_pred = lr4.predict(X_train)\ny_test_pred = lr4.predict(X_test)\n\n# To check the accuracy we can use a residual plot\ntrain_resid = y_train_pred - Y_train\ntest_resid = y_test_pred - Y_test\nplt.scatter(y_train_pred, train_resid, c=\"steelblue\", marker=\"o\", edgecolor=\"white\", label=\"Training data\")\nplt.scatter(y_test_pred, test_resid, c=\"limegreen\", marker=\"s\", edgecolor=\"white\", label=\"Test data\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend()\nplt.hlines(y=0, xmin=0, xmax=800, color=\"black\", lw=2)\nplt.show()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the prediction was perfect, all residuals should have collapsed on the black line, which is of course nearly impossible in real applications.\nIn our case, the plot shows that residuals are for the vast part concentrated along the black line, even though their distribution seem to be slightly skewed to the right, meaning that some relevant informations are still hidden inside the error term. ","metadata":{}},{"cell_type":"markdown","source":"Other metrics of accuracy are the Mean Squared Error and the R squared, which we can easily compute with ScikitLearn.","metadata":{}},{"cell_type":"code","source":"lr_MSE_train = mean_squared_error(Y_train, y_train_pred)\nlr_MSE_test = mean_squared_error(Y_test, y_test_pred)\n\nlr_r2_train = r2_score(Y_train, y_train_pred)\nlr_r2_test = r2_score(Y_test, y_test_pred)\n\nprint(\"TRAIN DATA: MSE = %.4f R^2 = %.4f\"%(lr_MSE_train, lr_r2_train))\nprint(\"TEST DATA: MSE = %.4f R^2 = %.4f\"%(lr_MSE_test, lr_r2_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The MSE in the test dataset is far superior to the one in the training dataset, which means the model overfits during the training phase.\nThe R^2 presents very low values for both training and test data: the model can hardly explain around 25% of the total variance of the data.","metadata":{}}]}